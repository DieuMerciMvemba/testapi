"""
ü¶à MaziShark API Backend - FastAPI
===================================
Backend scientifique pour visualiser les donn√©es oc√©anographiques et l'habitat potentiel des requins.

Endpoints principaux:
- GET / : Documentation de l'API
- GET /data/layers : Liste toutes les couches disponibles (metadata.json)
- GET /data/habitat : Retourne le GeoJSON de l'habitat potentiel
- GET /data/{layer_name} : Retourne les donn√©es NetCDF d'une couche sp√©cifique
- GET /data/{layer_name}/map : G√©n√®re une carte PNG d'une couche
- GET /predict : Pr√©diction de l'indice H √† un point lat/lon
- GET /hotspots : Retourne les zones √† fort potentiel (top 20%)
"""

from fastapi import FastAPI, HTTPException, Query, Path
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse, JSONResponse, StreamingResponse
from typing import Optional, Dict, Any
from functools import lru_cache
import os
import io
import json
import hashlib
import logging
import urllib.request
import urllib.error
import time
import datetime
import numpy as np
import xarray as xr
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# ============================================================================
# Configuration de l'application FastAPI
# ============================================================================

app = FastAPI(
    title="MaziShark API",
    version="2.0.0",
    description="API scientifique pour la visualisation des habitats de requins bas√©e sur PACE, MODIS, SST et SWOT"
)

# Configuration CORS - Permet les requ√™tes depuis n'importe quelle origine
# Important pour que le frontend React/Next.js puisse consommer l'API
cors_env = os.getenv("CORS_ALLOW_ORIGINS", "*")
allow_origins = [o.strip() for o in cors_env.split(",") if o.strip()] if cors_env else ["*"]
app.add_middleware(
    CORSMiddleware,
    allow_origins=allow_origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# ============================================================================
# Configuration des chemins de donn√©es
# ============================================================================

# R√©pertoire contenant toutes les donn√©es scientifiques (local uniquement)
PROCESSED_DATA_DIR = "processed_data"

# Cache des datasets NetCDF en m√©moire (√©vite de recharger √† chaque requ√™te)
_DATASET_CACHE: Dict[str, xr.Dataset] = {}

# Colormaps valides pour matplotlib
VALID_COLORMAPS = {
    "viridis", "plasma", "inferno", "magma", "cividis",
    "coolwarm", "RdYlBu", "RdYlGn", "Spectral", "jet",
    "hot", "cool", "spring", "summer", "autumn", "winter"
}


def get_data_path(filename: str) -> str:
    """
    Retourne le chemin absolu vers un fichier dans processed_data/.
    
    Args:
        filename: Nom du fichier
    
    Returns:
        Chemin local du fichier
        
    Raises:
        FileNotFoundError: Si le fichier n'existe pas
    """
    # Chercher dans processed_data/ (local uniquement)
    local_candidates = [
        os.path.join(os.path.dirname(__file__), "..", PROCESSED_DATA_DIR, filename),
        os.path.join(os.getcwd(), PROCESSED_DATA_DIR, filename),
        os.path.join(PROCESSED_DATA_DIR, filename),
    ]
    
    for path in local_candidates:
        abs_path = os.path.abspath(path)
        if os.path.exists(abs_path):
            logger.info(f"‚úÖ Fichier {filename} trouv√©: {abs_path}")
            return abs_path
    
    # Fichier introuvable
    raise FileNotFoundError(f"Fichier {filename} introuvable dans {PROCESSED_DATA_DIR}/")


# ============================================================================
# Fonctions utilitaires pour charger les donn√©es
# ============================================================================

@lru_cache(maxsize=1)
def load_metadata() -> Dict[str, Any]:
    """
    Charge metadata.json qui contient la liste de toutes les couches disponibles.
    Utilis√© par le frontend pour afficher dynamiquement les options de couches.
    
    Cache: R√©sultat mis en cache (ne charge qu'une seule fois).
    """
    try:
        path = get_data_path("metadata.json")
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur chargement metadata.json: {e}")


def load_netcdf(layer_name: str, use_cache: bool = True) -> xr.Dataset:
    """
    Charge un fichier NetCDF (.nc) avec xarray.
    Les fichiers NetCDF contiennent les donn√©es scientifiques multidimensionnelles (lat, lon, valeurs).
    
    Cache: Les datasets sont mis en cache en m√©moire pour √©viter de recharger √† chaque requ√™te.
    """
    # V√©rifier le cache d'abord
    if use_cache and layer_name in _DATASET_CACHE:
        return _DATASET_CACHE[layer_name]
    
    metadata = load_metadata()
    if layer_name not in metadata:
        raise HTTPException(status_code=404, detail=f"Couche '{layer_name}' introuvable dans metadata.json")
    
    filename = metadata[layer_name]["filename"]
    try:
        path = get_data_path(filename)
        ds = xr.open_dataset(path)
        
        # Mettre en cache
        if use_cache:
            _DATASET_CACHE[layer_name] = ds
        
        return ds
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur chargement {filename}: {e}")


def get_primary_variable(ds: xr.Dataset) -> str:
    """
    D√©tecte automatiquement la variable principale d'un dataset NetCDF.
    Ignore les coordonn√©es (lat, lon, time) et retourne la premi√®re variable de donn√©es.
    """
    coord_names = {"lat", "lon", "latitude", "longitude", "time", "x", "y"}
    data_vars = [v for v in ds.data_vars if v not in coord_names]
    if not data_vars:
        raise ValueError("Aucune variable de donn√©es trouv√©e dans le NetCDF")
    return data_vars[0]


def sample_data(data: np.ndarray, max_points: int = 10000) -> tuple[np.ndarray, int]:
    """
    √âchantillonne les donn√©es pour √©viter de surcharger la m√©moire et le r√©seau.
    R√©duit la r√©solution si n√©cessaire pour rester sous max_points.
    
    Returns:
        tuple: (donn√©es √©chantillonn√©es, facteur d'√©chantillonnage)
    """
    if data.size <= max_points:
        return data, 1
    
    # Calcul du facteur de sous-√©chantillonnage
    factor = int(np.ceil(np.sqrt(data.size / max_points)))
    return data[::factor, ::factor], factor


def get_lat_lon_arrays(ds: xr.Dataset) -> tuple[np.ndarray, np.ndarray]:
    """
    Extrait les tableaux de latitude et longitude d'un dataset xarray.
    G√®re les diff√©rents noms de coordonn√©es (lat/latitude, lon/longitude).
    Cherche dans coords ET dans variables.
    
    Returns:
        tuple: (lat_array, lon_array)
    """
    # D√©tecter le nom de la coordonn√©e latitude (dans coords ou variables)
    if "lat" in ds.coords:
        lat_arr = ds["lat"].values
    elif "latitude" in ds.coords:
        lat_arr = ds["latitude"].values
    elif "lat" in ds.variables:
        lat_arr = ds["lat"].values
    elif "latitude" in ds.variables:
        lat_arr = ds["latitude"].values
    else:
        raise ValueError(f"Aucune coordonn√©e latitude trouv√©e. Coords: {list(ds.coords)}, Variables: {list(ds.variables)}")
    
    # D√©tecter le nom de la coordonn√©e longitude (dans coords ou variables)
    if "lon" in ds.coords:
        lon_arr = ds["lon"].values
    elif "longitude" in ds.coords:
        lon_arr = ds["longitude"].values
    elif "lon" in ds.variables:
        lon_arr = ds["lon"].values
    elif "longitude" in ds.variables:
        lon_arr = ds["longitude"].values
    else:
        raise ValueError(f"Aucune coordonn√©e longitude trouv√©e. Coords: {list(ds.coords)}, Variables: {list(ds.variables)}")
    
    return lat_arr, lon_arr


def validate_coordinates(lat: float, lon: float, ds: xr.Dataset) -> None:
    """
    Valide que les coordonn√©es lat/lon sont dans la plage du dataset.
    L√®ve une HTTPException si hors limites.
    """
    lat_arr, lon_arr = get_lat_lon_arrays(ds)
    
    lat_min, lat_max = float(lat_arr.min()), float(lat_arr.max())
    lon_min, lon_max = float(lon_arr.min()), float(lon_arr.max())
    
    if not (lat_min <= lat <= lat_max):
        raise HTTPException(
            status_code=400,
            detail=f"Latitude {lat} hors limites. Plage valide: [{lat_min:.2f}, {lat_max:.2f}]"
        )
    
    if not (lon_min <= lon <= lon_max):
        raise HTTPException(
            status_code=400,
            detail=f"Longitude {lon} hors limites. Plage valide: [{lon_min:.2f}, {lon_max:.2f}]"
        )


# ============================================================================
# ENDPOINT 1: Page d'accueil de l'API
# ============================================================================

@app.get("/")
def root():
    """
    Point d'entr√©e de l'API - Documentation des endpoints disponibles.
    Utilis√© pour v√©rifier que l'API fonctionne et d√©couvrir les routes.
    """
    return {
        "name": "MaziShark API",
        "version": "2.0.0",
        "description": "API scientifique pour la visualisation des habitats de requins",
        "endpoints": {
            "/": "Cette page (documentation)",
            "/health": "V√©rification de l'√©tat de l'API",
            "/data/layers": "Liste toutes les couches disponibles (metadata.json)",
            "/data/habitat": "GeoJSON de l'habitat potentiel",
            "/data/{layer_name}": "Donn√©es JSON d'une couche NetCDF",
            "/data/{layer_name}/map": "Carte PNG d'une couche",
            "/predict?lat={lat}&lon={lon}": "Pr√©diction de l'indice H √† un point",
            "/hotspots": "Zones √† fort potentiel (top 20%)",
        },
        "available_layers": list(load_metadata().keys()),
        "data_directory": PROCESSED_DATA_DIR,
    }


@app.get("/health")
def health():
    """Endpoint de sant√© pour v√©rifier que l'API est op√©rationnelle."""
    return {"status": "ok", "message": "MaziShark API is running"}


# ============================================================================
# ENDPOINT 2: Liste des couches disponibles
# ============================================================================

@app.get("/data/layers")
def get_layers():
    """
    Retourne metadata.json qui liste toutes les couches scientifiques disponibles.
    
    Utilisation frontend:
    - Afficher dynamiquement les options de couches dans un menu d√©roulant
    - Conna√Ætre les noms de fichiers et tailles pour optimiser le chargement
    - Construire les URLs pour r√©cup√©rer chaque couche
    """
    try:
        metadata = load_metadata()
        return {
            "layers": metadata,
            "count": len(metadata),
            "description": "Couches oc√©anographiques disponibles pour visualisation"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# ============================================================================
# ENDPOINT 3: Habitat potentiel (GeoJSON)
# ============================================================================

@app.get("/data/habitat")
def get_habitat():
    """
    Retourne le GeoJSON de l'habitat potentiel des requins.
    
    Note: Le fichier habitat_potentiel.geojson n'existe pas encore dans processed_data/.
    Cet endpoint est pr√©par√© pour l'avenir. Pour l'instant, on utilise habitat_index_H.nc.
    
    Utilisation frontend:
    - Afficher les zones d'habitat sur une carte Leaflet avec react-leaflet
    - Styliser les polygones selon l'intensit√© de l'indice H
    """
    try:
        # V√©rifier si le GeoJSON existe
        geojson_path = get_data_path("habitat_potentiel.geojson")
        with open(geojson_path, "r", encoding="utf-8") as f:
            geojson_data = json.load(f)
        return geojson_data
    except FileNotFoundError:
        # Fallback: g√©n√©rer un GeoJSON minimal depuis habitat_index_H.nc
        try:
            ds = load_netcdf("habitat_index_H")
            return {
                "type": "FeatureCollection",
                "features": [],
                "message": "GeoJSON non disponible, utilisez /data/habitat_index_H pour les donn√©es brutes"
            }
        except:
            raise HTTPException(
                status_code=404,
                detail="Fichier habitat_potentiel.geojson introuvable. G√©n√©rez-le depuis le notebook."
            )


# ============================================================================
# ENDPOINT 4: Donn√©es NetCDF d'une couche sp√©cifique
# ============================================================================

@app.get("/data/{layer_name}")
def get_layer_data(
    layer_name: str = Path(..., description="Nom de la couche"),
    sample: bool = Query(True, description="√âchantillonner les donn√©es pour r√©duire la taille"),
    max_points: int = Query(10000, ge=100, le=100000, description="Nombre maximum de points (100-100000)")
):
    """
    Retourne les donn√©es d'une couche NetCDF en format JSON.
    
    Optimisations:
    - ‚úÖ Cache: Dataset mis en cache en m√©moire
    - ‚úÖ √âchantillonnage intelligent: R√©duit la r√©solution si n√©cessaire
    - ‚úÖ Validation: max_points ‚àà [100, 100000]
    
    Param√®tres:
    - layer_name: nom de la couche (pace_chlor_a, modis_chlor_a, sst_celsius, swot_ssh, habitat_index_H)
    - sample: si True, r√©duit la r√©solution pour optimiser la taille
    - max_points: nombre maximum de points √† retourner (100-100000)
    
    Utilisation frontend:
    - R√©cup√©rer les donn√©es pour affichage sur carte Leaflet
    - Cr√©er des heatmaps ou overlays color√©s
    - Afficher les valeurs dans des tooltips au survol
    
    Format de sortie:
    {
        "layer": "pace_chlor_a",
        "variable": "chlor_a",
        "lat": [array de latitudes],
        "lon": [array de longitudes],
        "values": [array 2D de valeurs],
        "units": "mg/m¬≥",
        "stats": {"min": 0.01, "max": 5.2, "mean": 0.8},
        "shape": [100, 150],
        "sampling_factor": 2
    }
    """
    try:
        # Chargement avec cache
        ds = load_netcdf(layer_name, use_cache=True)
        
        # D√©tection automatique de la variable principale
        var_name = get_primary_variable(ds)
        data_var = ds[var_name]
        
        # Extraction des coordonn√©es
        lat, lon = get_lat_lon_arrays(ds)
        values = data_var.values
        
        # √âchantillonnage si demand√©
        sampling_factor = 1
        if sample and values.size > max_points:
            values, sampling_factor = sample_data(values, max_points)
            # R√©duire aussi les coordonn√©es avec le m√™me facteur
            lat = lat[::sampling_factor]
            lon = lon[::sampling_factor]
        
        # Calcul des statistiques
        valid_values = values[np.isfinite(values)]
        stats = {
            "min": float(np.min(valid_values)) if valid_values.size > 0 else None,
            "max": float(np.max(valid_values)) if valid_values.size > 0 else None,
            "mean": float(np.mean(valid_values)) if valid_values.size > 0 else None,
        }
        
        # Conversion en listes pour JSON
        return {
            "layer": layer_name,
            "variable": var_name,
            "lat": lat.tolist(),
            "lon": lon.tolist(),
            "values": np.where(np.isfinite(values), values, None).tolist(),
            "units": data_var.attrs.get("units", "unknown"),
            "long_name": data_var.attrs.get("long_name", var_name),
            "stats": stats,
            "shape": list(values.shape),
            "sampled": sample,
            "sampling_factor": sampling_factor,
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur traitement {layer_name}: {e}")


# ============================================================================
# ENDPOINT 5: Carte PNG d'une couche
# ============================================================================

@app.get("/data/{layer_name}/map")
def get_layer_map(
    layer_name: str = Path(..., description="Nom de la couche"),
    cmap: str = Query("viridis", description="Colormap matplotlib")
):
    """
    G√©n√®re une carte PNG d'une couche NetCDF avec matplotlib.
    
    Optimisations:
    - ‚úÖ Cache: Dataset mis en cache en m√©moire
    - ‚úÖ Streaming: Retourne le PNG directement (pas de fichier temporaire)
    - ‚úÖ Validation: V√©rifie que la colormap est valide
    
    Utilisation frontend:
    - Afficher comme ImageOverlay sur Leaflet
    - T√©l√©charger la carte en haute r√©solution
    - Pr√©visualisation rapide sans charger toutes les donn√©es
    """
    # Validation de la colormap
    if cmap not in VALID_COLORMAPS:
        raise HTTPException(
            status_code=400,
            detail=f"Colormap '{cmap}' invalide. Valides: {', '.join(sorted(VALID_COLORMAPS))}"
        )
    
    try:
        # Chargement avec cache
        ds = load_netcdf(layer_name, use_cache=True)
        var_name = get_primary_variable(ds)
        data_var = ds[var_name]
        
        lat, lon = get_lat_lon_arrays(ds)
        values = data_var.values
        
        # Cr√©ation de la figure
        fig, ax = plt.subplots(figsize=(12, 8))
        im = ax.pcolormesh(lon, lat, values, cmap=cmap, shading="auto")
        
        # Titre et labels
        title = data_var.attrs.get("long_name", layer_name)
        units = data_var.attrs.get("units", "")
        ax.set_title(f"{title}", fontsize=14, fontweight="bold")
        ax.set_xlabel("Longitude", fontsize=12)
        ax.set_ylabel("Latitude", fontsize=12)
        
        # Colorbar
        cbar = plt.colorbar(im, ax=ax, label=f"{units}")
        cbar.ax.tick_params(labelsize=10)
        
        # Sauvegarde en m√©moire (streaming, pas de fichier temporaire)
        buf = io.BytesIO()
        plt.tight_layout()
        fig.savefig(buf, format="png", dpi=150, bbox_inches="tight")
        plt.close(fig)
        buf.seek(0)
        
        # Retour direct en streaming (√©vite les fichiers temporaires et conflits)
        return StreamingResponse(
            buf,
            media_type="image/png",
            headers={"Content-Disposition": f"inline; filename={layer_name}_map.png"}
        )
    
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur g√©n√©ration carte {layer_name}: {e}")


# ============================================================================
# ENDPOINT 6: Pr√©diction au point (lat, lon)
# ============================================================================

@app.get("/predict")
def predict(
    lat: float = Query(..., ge=-90, le=90, description="Latitude du point (-90 √† 90)"),
    lon: float = Query(..., ge=-180, le=180, description="Longitude du point (-180 √† 180)")
):
    """
    Retourne l'indice d'habitat H au point le plus proche (nearest neighbor).
    
    Optimisations:
    - ‚úÖ Cache: Dataset mis en cache en m√©moire
    - ‚úÖ Validation: V√©rifie que lat/lon sont dans les limites du dataset
    - ‚úÖ Validation: V√©rifie que lat ‚àà [-90, 90] et lon ‚àà [-180, 180]
    
    Utilisation frontend:
    - Clic sur la carte pour obtenir la valeur H
    - Afficher dans un popup ou panneau lat√©ral
    - √âvaluer le potentiel d'habitat √† un endroit pr√©cis
    """
    try:
        # Chargement avec cache
        ds = load_netcdf("habitat_index_H", use_cache=True)
        
        # Validation des coordonn√©es (v√©rifie qu'elles sont dans la plage du dataset)
        validate_coordinates(lat, lon, ds)
        
        H = ds["H_index"]
        lat_arr = ds["lat"].values
        lon_arr = ds["lon"].values
        
        # Recherche du voisin le plus proche
        i = int(np.abs(lat_arr - lat).argmin())
        j = int(np.abs(lon_arr - lon).argmin())
        
        val = float(H.values[i, j]) if np.isfinite(H.values[i, j]) else None
        
        return {
            "lat": lat,
            "lon": lon,
            "H_index": val,
            "nearest_lat": float(lat_arr[i]),
            "nearest_lon": float(lon_arr[j]),
            "grid_indices": {"i": i, "j": j},
            "interpretation": "High potential" if val and val > 0.7 else "Moderate potential" if val and val > 0.4 else "Low potential"
        }
    
    except HTTPException:
        raise
    except FileNotFoundError:
        raise HTTPException(status_code=404, detail="Fichier habitat_index_H.nc introuvable")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur pr√©diction: {e}")


# ============================================================================
# ENDPOINT 7: Hotspots (zones √† fort potentiel)
# ============================================================================

@app.get("/hotspots")
def get_hotspots():
    """
    Retourne les zones √† fort potentiel d'habitat (top 20% des valeurs H).
    
    Utilisation frontend:
    - Afficher des marqueurs sur les zones prioritaires
    - Cr√©er une liste de recommandations pour les chercheurs
    - Filtrer la carte pour ne montrer que les zones int√©ressantes
    """
    try:
        # V√©rifier si le CSV existe
        csv_path = get_data_path("hotspots_H_top20.csv")
        
        # Lecture simple du CSV
        hotspots = []
        with open(csv_path, "r", encoding="utf-8") as f:
            lines = f.readlines()
            headers = lines[0].strip().split(",")
            
            for line in lines[1:]:
                values = line.strip().split(",")
                hotspot = {headers[i]: float(values[i]) if i > 0 else int(values[i]) for i in range(len(headers))}
                hotspots.append(hotspot)
        
        return {
            "hotspots": hotspots,
            "count": len(hotspots),
            "description": "Top 20% des zones avec le plus fort indice d'habitat"
        }
    
    except FileNotFoundError:
        raise HTTPException(status_code=404, detail="Fichier hotspots_H_top20.csv introuvable")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur chargement hotspots: {e}")


# ============================================================================
# ENDPOINT LEGACY: Compatibilit√© avec l'ancien frontend
# ============================================================================

@app.get("/meta")
def meta():
    """Endpoint de compatibilit√© avec l'ancien frontend."""
    try:
        ds = load_netcdf("habitat_index_H")
        lat = ds["lat"].values
        lon = ds["lon"].values
        return {
            "lat": {"size": int(lat.size), "min": float(lat.min()), "max": float(lat.max())},
            "lon": {"size": int(lon.size), "min": float(lon.min()), "max": float(lon.max())},
        }
    except:
        raise HTTPException(status_code=404, detail="Donn√©es habitat introuvables")


@app.get("/map")
def map_legacy():
    """Endpoint de compatibilit√© - redirige vers /data/habitat_index_H/map"""
    return get_layer_map("habitat_index_H")


@app.get("/data/habitat")
def get_habitat_geojson(threshold: float = 0.1, max_features: int = 1000, format: str = "geojson"):
    """
    G√©n√®re un GeoJSON des zones d'habitat potentiel √† partir de habitat_index_H.nc

    Args:
        threshold: Seuil H_index minimum (0.0-1.0)
        max_features: Nombre maximum de polygones √† g√©n√©rer
        format: Format de sortie ("geojson" ou "json")

    Retourne uniquement les cellules avec H_index > threshold (zones favorables)
    """
    try:
        # Validation des param√®tres
        if not 0.0 <= threshold <= 1.0:
            raise HTTPException(status_code=400, detail="Threshold doit √™tre entre 0.0 et 1.0")
        if max_features < 1 or max_features > 50000:
            raise HTTPException(status_code=400, detail="max_features doit √™tre entre 1 et 50000")

        # Charger le fichier NetCDF
        ds = load_netcdf("habitat_index_H", use_cache=True)

        # Extraire les donn√©es
        lat, lon = get_lat_lon_arrays(ds)
        h_values = ds["H_index"].values

        # Filtrer les zones favorables (H > threshold)
        mask = h_values > threshold

        # Cr√©er les features GeoJSON
        features = []
        lat_indices, lon_indices = np.where(mask)

        # Limiter le nombre de features
        total_features = min(len(lat_indices), max_features)

        for idx in range(total_features):
            i, j = lat_indices[idx], lon_indices[idx]
            lat_center = float(lat[i])
            lon_center = float(lon[j])
            h_value = float(h_values[i, j])

            # Cr√©er un petit polygone autour du point central
            # R√©solution approximative: ~0.25¬∞ par cellule
            half_res_lat = 0.125  # 0.25¬∞ / 2
            half_res_lon = 0.125

            # Coordonn√©es du polygone (rectangle autour du point)
            polygon_coords = [
                [lon_center - half_res_lon, lat_center - half_res_lat],  # SW
                [lon_center + half_res_lon, lat_center - half_res_lat],  # SE
                [lon_center + half_res_lon, lat_center + half_res_lat],  # NE
                [lon_center - half_res_lon, lat_center + half_res_lat],  # NW
                [lon_center - half_res_lon, lat_center - half_res_lat]   # Fermer le polygone
            ]

            feature = {
                "type": "Feature",
                "geometry": {
                    "type": "Polygon",
                    "coordinates": [polygon_coords]
                },
                "properties": {
                    "H_index": h_value,
                    "latitude": lat_center,
                    "longitude": lon_center,
                    "habitat_potential": "high" if h_value > 0.8 else "medium",
                    "confidence": min(h_value * 100, 100)  # Pourcentage
                }
            }
            features.append(feature)

        # Cr√©er le GeoJSON complet
        geojson = {
            "type": "FeatureCollection",
            "features": features,
            "metadata": {
                "total_zones": len(features),
                "total_available": len(lat_indices),
                "threshold": threshold,
                "max_features": max_features,
                "generated_at": datetime.datetime.now().isoformat(),
                "source": "habitat_index_H.nc"
            }
        }

        # Headers pour GeoJSON
        headers = {"Content-Type": "application/geo+json"}

        return JSONResponse(content=geojson, headers=headers)

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Erreur g√©n√©ration GeoJSON habitat: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Erreur g√©n√©ration GeoJSON: {str(e)}"
        )


